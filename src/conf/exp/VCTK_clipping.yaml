num_workers: 1

task: "declipping"

#training related parameters
batch_size: 8
optimizer:
  _target_: torch.optim.Adam
  lr: 1e-4
  betas: [0.9, 0.999]

#lr_rampup_it: 10000 # 'Learning rate rampup duration'
lr_rampup_it: 10 # 'Learning rate rampup duration'

#
cudnn_bench: True 

segment_length: 65536
fs: 16000

cudnn_autotuner: False #autotune cudnn for the best performance

max_steps: 10000000 #maximum number of steps to train


#gradient clipping
use_grad_clip: True
max_grad_norm: 1 #1

#EMA
ema_rate: 0.9999 #0.9999
ema_rampup: 10000 #10000

conditional: True

#regularization for the encoder
beta: 0.0 
beta_reverb: 10.0 

l2_norm: False
similarity: "cossim"
